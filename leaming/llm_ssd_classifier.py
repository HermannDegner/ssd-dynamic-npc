"""
LLMã‚’ä½¿ã£ãŸSSDçŠ¶æ…‹åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ 
æ—¢å­˜ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰SSDçŠ¶æ…‹ã‚’æŠ½å‡ºã—ã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•ç”Ÿæˆ
"""

import json
import random
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, asdict
import asyncio
from datetime import datetime

# å®Ÿéš›ã®å®Ÿè£…ã§ã¯ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã‚’ä½¿ç”¨
# from openai import OpenAI  # OpenAI API
# from anthropic import Anthropic  # Claude API
# import ollama  # ãƒ­ãƒ¼ã‚«ãƒ«LLM

@dataclass
class SSDStateFromText:
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸSSDçŠ¶æ…‹"""
    # ç‰©ç†å±¤
    weather_condition: str = "unknown"
    threat_level: float = 0.0
    time_of_day: str = "unknown"
    location_type: str = "unknown"
    
    # åŸºå±¤ï¼ˆ0-1ã®å€¤ï¼‰
    comfort_level: float = 0.5
    social_need: float = 0.5
    exploration_desire: float = 0.5
    creation_urge: float = 0.5
    recognition_need: float = 0.5
    
    # æ…£æ€§ãƒ»è¨˜æ†¶
    mentioned_habits: List[str] = None
    emotional_memories: List[str] = None
    
    # ãƒ¡ã‚¿æƒ…å ±
    confidence_score: float = 0.0
    personality_indicators: List[str] = None
    
    def __post_init__(self):
        if self.mentioned_habits is None:
            self.mentioned_habits = []
        if self.emotional_memories is None:
            self.emotional_memories = []
        if self.personality_indicators is None:
            self.personality_indicators = []

class LLMSSDClassifier:
    """LLMã‚’ä½¿ã£ãŸSSDçŠ¶æ…‹åˆ†é¡å™¨"""
    
    def __init__(self, model_type: str = "mock"):
        self.model_type = model_type
        self.classification_prompt = self._create_classification_prompt()
        
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯é©åˆ‡ãªã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
        # self.client = OpenAI() or Anthropic() or ollama
        
    def _create_classification_prompt(self) -> str:
        """SSDçŠ¶æ…‹åˆ†é¡ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ"""
        return """
ã‚ãªãŸã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®å†…éƒ¨çŠ¶æ…‹ã‚’åˆ†æã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã‚“ã§ã€ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®SSDï¼ˆæ§‹é€ ä¸»è¦³åŠ›å­¦ï¼‰çŠ¶æ…‹ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚

## åˆ†æé …ç›®

### ç‰©ç†å±¤çŠ¶æ…‹
- weather_condition: å¤©å€™çŠ¶æ³ (sunny/cloudy/rainy/stormy/unknown)
- threat_level: è„…å¨ãƒ¬ãƒ™ãƒ« (0.0-1.0)
- time_of_day: æ™‚é–“å¸¯ (morning/day/evening/night/unknown)
- location_type: å ´æ‰€ã‚¿ã‚¤ãƒ— (home/village/nature/shop/unknown)

### åŸºå±¤çŠ¶æ…‹ï¼ˆå„é …ç›®0.0-1.0ã§è©•ä¾¡ï¼‰
- comfort_level: å®‰å¿ƒãƒ»å¿«é©ã•ã®ãƒ¬ãƒ™ãƒ«
- social_need: ç¤¾äº¤ã¸ã®æ¬²æ±‚
- exploration_desire: æ¢ç´¢ãƒ»æ–°ä½“é¨“ã¸ã®æ¬²æ±‚
- creation_urge: å‰µé€ ãƒ»è¡¨ç¾ã¸ã®è¡å‹•
- recognition_need: æ‰¿èªãƒ»è©•ä¾¡ã¸ã®æ¬²æ±‚

### æ…£æ€§ãƒ»è¨˜æ†¶
- mentioned_habits: è¨€åŠã•ã‚ŒãŸç¿’æ…£ãƒ»è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³
- emotional_memories: æ„Ÿæƒ…çš„ãªè¨˜æ†¶ãƒ»éå»ã®çµŒé¨“
- personality_indicators: æ€§æ ¼ã‚’ç¤ºã™ç‰¹å¾´

### ãƒ¡ã‚¿æƒ…å ±
- confidence_score: åˆ†æã®ç¢ºä¿¡åº¦ (0.0-1.0)

## å‡ºåŠ›å½¢å¼
JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š

```json
{
    "weather_condition": "sunny",
    "threat_level": 0.0,
    "time_of_day": "morning",
    "location_type": "village",
    "comfort_level": 0.8,
    "social_need": 0.6,
    "exploration_desire": 0.3,
    "creation_urge": 0.2,
    "recognition_need": 0.4,
    "mentioned_habits": ["morning_walk", "coffee_routine"],
    "emotional_memories": ["happy_childhood", "recent_success"],
    "personality_indicators": ["optimistic", "friendly"],
    "confidence_score": 0.9
}
```

åˆ†æå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ:
"""

    async def classify_text(self, text: str) -> SSDStateFromText:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰SSDçŠ¶æ…‹ã‚’åˆ†é¡"""
        
        # å®Ÿéš›ã®å®Ÿè£…ä¾‹ï¼ˆãƒ¢ãƒƒã‚¯ï¼‰
        if self.model_type == "mock":
            return self._mock_classification(text)
        
        # å®Ÿéš›ã®LLMå‘¼ã³å‡ºã—ä¾‹
        # response = await self._call_llm(text)
        # return self._parse_llm_response(response)
    
    def _mock_classification(self, text: str) -> SSDStateFromText:
        """ãƒ¢ãƒƒã‚¯åˆ†é¡ï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰"""
        
        # ç°¡å˜ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹åˆ†æï¼ˆå®Ÿéš›ã¯LLMãŒæ‹…å½“ï¼‰
        text_lower = text.lower()
        
        # å¤©å€™æ¤œå‡º
        weather_keywords = {
            "sunny": ["æ™´ã‚Œ", "å¤ªé™½", "æ˜ã‚‹ã„"],
            "rainy": ["é›¨", "æ¿¡ã‚Œã‚‹", "å‚˜"],
            "stormy": ["åµ", "é›·", "å¼·é¢¨"],
            "cloudy": ["æ›‡ã‚Š", "é›²"]
        }
        
        weather = "unknown"
        for condition, keywords in weather_keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                weather = condition
                break
        
        # æ„Ÿæƒ…ãƒ»æ¬²æ±‚ã®æ¨å®š
        comfort_indicators = ["å®‰å¿ƒ", "å¿«é©", "ãƒªãƒ©ãƒƒã‚¯ã‚¹", "å¹³å’Œ"]
        social_indicators = ["è©±ã™", "å‹é”", "ä¸€ç·’", "ä¼šè©±", "ã¿ã‚“ãª"]
        exploration_indicators = ["æ¢ç´¢", "å†’é™º", "æ–°ã—ã„", "ç™ºè¦‹", "è©¦ã™"]
        creation_indicators = ["ä½œã‚‹", "æã", "æ›¸ã", "å‰µä½œ", "è¡¨ç¾"]
        recognition_indicators = ["è¤’ã‚ã‚‰ã‚Œ", "èªã‚ã‚‰ã‚Œ", "è©•ä¾¡", "ã™ã”ã„"]
        
        comfort_level = min(1.0, sum(0.2 for word in comfort_indicators if word in text_lower))
        social_need = min(1.0, sum(0.2 for word in social_indicators if word in text_lower))
        exploration_desire = min(1.0, sum(0.2 for word in exploration_indicators if word in text_lower))
        creation_urge = min(1.0, sum(0.2 for word in creation_indicators if word in text_lower))
        recognition_need = min(1.0, sum(0.2 for word in recognition_indicators if word in text_lower))
        
        # è„…å¨ãƒ¬ãƒ™ãƒ«
        threat_keywords = ["å±é™º", "æ€–ã„", "ä¸å®‰", "å¿ƒé…", "åµ"]
        threat_level = min(1.0, sum(0.3 for word in threat_keywords if word in text_lower))
        
        # æ™‚é–“å¸¯æ¤œå‡º
        time_keywords = {
            "morning": ["æœ", "åˆå‰"],
            "day": ["æ˜¼", "åˆå¾Œ", "æ—¥ä¸­"],
            "evening": ["å¤•æ–¹", "å¤•æš®ã‚Œ"],
            "night": ["å¤œ", "å¤œä¸­"]
        }
        
        time_of_day = "unknown"
        for period, keywords in time_keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                time_of_day = period
                break
        
        return SSDStateFromText(
            weather_condition=weather,
            threat_level=threat_level,
            time_of_day=time_of_day,
            location_type="village",  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            comfort_level=comfort_level,
            social_need=social_need,
            exploration_desire=exploration_desire,
            creation_urge=creation_urge,
            recognition_need=recognition_need,
            mentioned_habits=[],
            emotional_memories=[],
            personality_indicators=[],
            confidence_score=0.7  # ãƒ¢ãƒƒã‚¯ãªã®ã§ä½ã‚
        )
    
    async def _call_llm(self, text: str) -> str:
        """å®Ÿéš›ã®LLMå‘¼ã³å‡ºã—ï¼ˆå®Ÿè£…ä¾‹ï¼‰"""
        
        # OpenAI GPT-4ã®å ´åˆ
        """
        response = await self.client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": self.classification_prompt},
                {"role": "user", "content": text}
            ],
            temperature=0.1
        )
        return response.choices[0].message.content
        """
        
        # Claude 3.5 Sonnetã®å ´åˆ
        """
        response = await self.client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=1000,
            messages=[
                {"role": "user", "content": f"{self.classification_prompt}\n\n{text}"}
            ]
        )
        return response.content[0].text
        """
        
        # Ollamaãƒ­ãƒ¼ã‚«ãƒ«ã®å ´åˆ
        """
        response = ollama.chat(
            model='llama3.1',
            messages=[
                {'role': 'system', 'content': self.classification_prompt},
                {'role': 'user', 'content': text}
            ]
        )
        return response['message']['content']
        """
        
        return ""  # ãƒ¢ãƒƒã‚¯ç”¨
    
    def _parse_llm_response(self, response: str) -> SSDStateFromText:
        """LLMå¿œç­”ã‚’ãƒ‘ãƒ¼ã‚¹"""
        try:
            # JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
            response_clean = response.strip()
            if "```json" in response_clean:
                start = response_clean.find("```json") + 7
                end = response_clean.find("```", start)
                response_clean = response_clean[start:end].strip()
            
            data = json.loads(response_clean)
            return SSDStateFromText(**data)
            
        except Exception as e:
            print(f"ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿”ã™
            return SSDStateFromText()

class SSDTrainingDataGenerator:
    """SSDå­¦ç¿’ãƒ‡ãƒ¼ã‚¿è‡ªå‹•ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.classifier = LLMSSDClassifier()
        
        # ã©ã†ã¶ã¤ã®æ£®é¢¨ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
        self.sample_texts = [
            "ä»Šæ—¥ã¯æœã‹ã‚‰é›¨ãŒé™ã£ã¦ã„ã¦ã€ã¡ã‚‡ã£ã¨æ†‚é¬±ãªæ°—åˆ†ã§ã™ã€‚å®¶ã§ã‚†ã£ãã‚Šèª­æ›¸ã§ã‚‚ã—ã‚ˆã†ã‹ãªã€‚",
            "ãŠå¤©æ°—ãŒè‰¯ãã¦ã€ã¿ã‚“ãªã§ãƒ”ã‚¯ãƒ‹ãƒƒã‚¯ã«è¡Œãã¾ã—ãŸï¼ã¨ã¦ã‚‚æ¥½ã—ã‹ã£ãŸã§ã™ã€‚",
            "æ–°ã—ã„å ´æ‰€ã‚’æ¢æ¤œã—ã¦ã¿ãŸããªã‚Šã¾ã—ãŸã€‚ä½•ã‹é¢ç™½ã„ç™ºè¦‹ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚",
            "ä»Šæ—¥ã¯å‰µä½œæ´»å‹•ã«é›†ä¸­ã—ãŸã„æ°—åˆ†ã§ã™ã€‚æ–°ã—ã„ä½œå“ã‚’ä½œã£ã¦ã¿ã‚ˆã†ã¨æ€ã„ã¾ã™ã€‚",
            "å‹é”ã«è¤’ã‚ã‚‰ã‚Œã¦ã€ã¨ã¦ã‚‚å¬‰ã—ã„æ°—æŒã¡ã«ãªã‚Šã¾ã—ãŸã€‚é ‘å¼µã£ãŸç”²æ–ãŒã‚ã‚Šã¾ã—ãŸã€‚",
            "å¤œã«ãªã£ã¦å°‘ã—ä¸å®‰ã«ãªã£ã¦ãã¾ã—ãŸã€‚æ—©ãå®¶ã«å¸°ã‚ã†ã¨æ€ã„ã¾ã™ã€‚",
            "æ¯æœã®ã‚³ãƒ¼ãƒ’ãƒ¼ãŒæ—¥èª²ã«ãªã£ã¦ã„ã¾ã™ã€‚ã“ã®æ™‚é–“ãŒã¨ã¦ã‚‚è½ã¡ç€ãã¾ã™ã€‚",
            "æ‘ã®ãŠç¥­ã‚Šã§ã¿ã‚“ãªã¨è¸Šã‚Šã¾ã—ãŸã€‚ã“ã†ã„ã†æ™‚é–“ãŒã¨ã¦ã‚‚å¤§åˆ‡ã ã¨æ„Ÿã˜ã¾ã™ã€‚"
        ]
    
    async def generate_training_data(self, num_samples: int = 100) -> List[Dict]:
        """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®è‡ªå‹•ç”Ÿæˆ"""
        
        training_data = []
        
        print(f"SSDå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’{num_samples}ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆä¸­...")
        
        # æ—¢å­˜ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã®å‡¦ç†
        for text in self.sample_texts:
            ssd_state = await self.classifier.classify_text(text)
            
            training_data.append({
                "input_text": text,
                "ssd_state": asdict(ssd_state),
                "timestamp": datetime.now().isoformat(),
                "source": "sample"
            })
        
        # è¿½åŠ ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ
        additional_samples = num_samples - len(self.sample_texts)
        if additional_samples > 0:
            for i in range(additional_samples):
                # ãƒ©ãƒ³ãƒ€ãƒ ãªãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ
                variation_text = self._generate_text_variation()
                ssd_state = await self.classifier.classify_text(variation_text)
                
                training_data.append({
                    "input_text": variation_text,
                    "ssd_state": asdict(ssd_state),
                    "timestamp": datetime.now().isoformat(),
                    "source": "generated"
                })
        
        print(f"ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {len(training_data)}ã‚µãƒ³ãƒ—ãƒ«")
        return training_data
    
    def _generate_text_variation(self) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ"""
        
        # ç°¡å˜ãªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ç”Ÿæˆï¼ˆå®Ÿéš›ã¯LLMã§è¡Œã†ï¼‰
        weather_options = ["æ™´ã‚Œã¦", "é›¨ãŒé™ã£ã¦", "æ›‡ã‚Šã§", "åµã§"]
        time_options = ["æœ", "æ˜¼", "å¤•æ–¹", "å¤œ"]
        activity_options = ["æ•£æ­©", "èª­æ›¸", "æ–™ç†", "æƒé™¤", "çµµã‚’æã", "å‹é”ã¨è©±ã™"]
        emotion_options = ["æ¥½ã—ã„", "æ‚²ã—ã„", "èˆˆå¥®ã—ãŸ", "è½ã¡ç€ã„ãŸ", "ä¸å®‰ãª"]
        
        weather = random.choice(weather_options)
        time = random.choice(time_options)
        activity = random.choice(activity_options)
        emotion = random.choice(emotion_options)
        
        templates = [
            f"{time}ã«{weather}ã„ã‚‹ã®ã§ã€{activity}ã‚’ã—ãŸã„æ°—åˆ†ã§ã™ã€‚{emotion}æ°—æŒã¡ã«ãªã‚Šã¾ã™ã€‚",
            f"ä»Šæ—¥ã¯{weather}ã„ã¾ã™ã€‚{activity}ã‚’ã—ã¦{emotion}æ™‚é–“ã‚’éã”ã—ã¾ã—ãŸã€‚",
            f"{time}ã®æ™‚é–“å¸¯ã«{activity}ã‚’ã™ã‚‹ã®ãŒå¥½ãã§ã™ã€‚{weather}ã„ã‚‹æ—¥ã¯ç‰¹ã«{emotion}æ°—åˆ†ã«ãªã‚Šã¾ã™ã€‚"
        ]
        
        return random.choice(templates)
    
    def save_training_data(self, data: List[Dict], filename: str = "ssd_training_data.json"):
        """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’{filename}ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    def load_training_data(self, filename: str = "ssd_training_data.json") -> List[Dict]:
        """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿"""
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’{filename}ã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(data)}ã‚µãƒ³ãƒ—ãƒ«")
            return data
        except FileNotFoundError:
            print(f"ãƒ•ã‚¡ã‚¤ãƒ«{filename}ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return []

class SSDDataAnalyzer:
    """SSDå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®åˆ†æå™¨"""
    
    def analyze_data_distribution(self, training_data: List[Dict]) -> Dict:
        """ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®åˆ†æ"""
        
        if not training_data:
            return {}
        
        # åŸºå±¤çŠ¶æ…‹ã®åˆ†å¸ƒ
        comfort_levels = [item["ssd_state"]["comfort_level"] for item in training_data]
        social_needs = [item["ssd_state"]["social_need"] for item in training_data]
        exploration_desires = [item["ssd_state"]["exploration_desire"] for item in training_data]
        creation_urges = [item["ssd_state"]["creation_urge"] for item in training_data]
        recognition_needs = [item["ssd_state"]["recognition_need"] for item in training_data]
        
        # å¤©å€™åˆ†å¸ƒ
        weather_dist = {}
        for item in training_data:
            weather = item["ssd_state"]["weather_condition"]
            weather_dist[weather] = weather_dist.get(weather, 0) + 1
        
        # æ™‚é–“å¸¯åˆ†å¸ƒ
        time_dist = {}
        for item in training_data:
            time_period = item["ssd_state"]["time_of_day"]
            time_dist[time_period] = time_dist.get(time_period, 0) + 1
        
        analysis = {
            "total_samples": len(training_data),
            "basal_states": {
                "comfort_avg": sum(comfort_levels) / len(comfort_levels),
                "social_avg": sum(social_needs) / len(social_needs),
                "exploration_avg": sum(exploration_desires) / len(exploration_desires),
                "creation_avg": sum(creation_urges) / len(creation_urges),
                "recognition_avg": sum(recognition_needs) / len(recognition_needs)
            },
            "weather_distribution": weather_dist,
            "time_distribution": time_dist,
            "confidence_scores": [item["ssd_state"]["confidence_score"] for item in training_data]
        }
        
        return analysis
    
    def print_analysis(self, analysis: Dict):
        """åˆ†æçµæœã®è¡¨ç¤º"""
        print("\n=== SSDå­¦ç¿’ãƒ‡ãƒ¼ã‚¿åˆ†æçµæœ ===")
        print(f"ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {analysis['total_samples']}")
        
        print("\nã€åŸºå±¤çŠ¶æ…‹å¹³å‡å€¤ã€‘")
        basal = analysis['basal_states']
        print(f"  å®‰å¿ƒãƒ¬ãƒ™ãƒ«: {basal['comfort_avg']:.3f}")
        print(f"  ç¤¾äº¤æ¬²æ±‚: {basal['social_avg']:.3f}")
        print(f"  æ¢ç´¢æ¬²æ±‚: {basal['exploration_avg']:.3f}")
        print(f"  å‰µé€ è¡å‹•: {basal['creation_avg']:.3f}")
        print(f"  æ‰¿èªæ¬²æ±‚: {basal['recognition_avg']:.3f}")
        
        print("\nã€å¤©å€™åˆ†å¸ƒã€‘")
        for weather, count in analysis['weather_distribution'].items():
            percentage = (count / analysis['total_samples']) * 100
            print(f"  {weather}: {count}ä»¶ ({percentage:.1f}%)")
        
        print("\nã€æ™‚é–“å¸¯åˆ†å¸ƒã€‘")
        for time_period, count in analysis['time_distribution'].items():
            percentage = (count / analysis['total_samples']) * 100
            print(f"  {time_period}: {count}ä»¶ ({percentage:.1f}%)")
        
        avg_confidence = sum(analysis['confidence_scores']) / len(analysis['confidence_scores'])
        print(f"\nå¹³å‡ç¢ºä¿¡åº¦: {avg_confidence:.3f}")

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ¤– LLMãƒ™ãƒ¼ã‚¹SSDçŠ¶æ…‹åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 50)
    
    # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå™¨ã®åˆæœŸåŒ–
    generator = SSDTrainingDataGenerator()
    analyzer = SSDDataAnalyzer()
    
    # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
    training_data = await generator.generate_training_data(num_samples=20)
    
    # ãƒ‡ãƒ¼ã‚¿åˆ†æ
    analysis = analyzer.analyze_data_distribution(training_data)
    analyzer.print_analysis(analysis)
    
    # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    generator.save_training_data(training_data)
    
    # å€‹åˆ¥ã‚µãƒ³ãƒ—ãƒ«ã®è¡¨ç¤º
    print("\n=== ã‚µãƒ³ãƒ—ãƒ«ä¾‹ ===")
    for i, sample in enumerate(training_data[:3]):
        print(f"\nã€ã‚µãƒ³ãƒ—ãƒ«{i+1}ã€‘")
        print(f"å…¥åŠ›: {sample['input_text']}")
        print(f"å¤©å€™: {sample['ssd_state']['weather_condition']}")
        print(f"è„…å¨: {sample['ssd_state']['threat_level']:.2f}")
        print(f"å®‰å¿ƒ: {sample['ssd_state']['comfort_level']:.2f}")
        print(f"ç¤¾äº¤: {sample['ssd_state']['social_need']:.2f}")
        print(f"ç¢ºä¿¡åº¦: {sample['ssd_state']['confidence_score']:.2f}")
    
    print("\nâœ¨ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†ï¼")
    print("ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦æœ¬æ ¼çš„ãªSSDãƒ¬ãƒ³ã‚ºå­¦ç¿’ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚")

if __name__ == "__main__":
    asyncio.run(main())
